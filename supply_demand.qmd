---
title: "The Supply and Demand of Fact vs. Opinion in Presidential Tweets"
author: 
  - Thad Kousser^[Professor, Department of Political Science, UC San Diego.] 
  - Stan Oklobdzija^[Assistant Professor, Department of Political Science, Tulane University.]"
format: pdf
pdf-engine: pdflatex
indent: "4m"
linestretch: 2
editor: visual
bibliography: presidential_tweets.bib
abstract: | 
  Do presidential candidates use social media to lay out the factual basis for their positions, or do they primarily communicate their values and opinions? We investigate this question by using human categorizations of over 8,000 tweets to train machine learning algorithms to code all of the tweets sent by all XX candidates during the 2016 and 2020 presidential election campaigns. We argue that politicians should supply more opinions than facts, in order to match voter demand for opinion over fact and increase audience engagement with their tweets.
  In a descriptive analysis, we chart the flow of factual claims -- many of which happen to be false, according to prominent fact-checking websites -- versus opinion over the course of the campaign and across candidates.
---

\newpage

## Introduction

Among the many unprecedented features of the 2016 and 2020 presidential elections, two stand out: the ascendance of Twitter as a medium of direct communication between candidates and the public[^1] and a declining adherence to facts in elite political rhetoric. How did these trends interact in the discourse of presidential candidates, and how did their audiences react? When presidential aspirants tweeted, did they seek to inform their followers by making factual statements or to persuade them by laying out their opinions? Did those who read their tweets chose to spread facts across the Twittersphere by retweeting such statements, or was it opinion that was most likely to generate engagement on social media?

[^1]: For evidence of the ascendance of Twitter, see Mike Isaac and Sydney Ember, "For Election Day Influence, Twitter Ruled Social Media," New York Times, November 8, 2016 or the observations of Dan Pfeiffer, Barack Obama's former communications director, paraphrased as "future White Houses will spend their time figuring out how to connect with voters on social media," in Joe Garofoli, "After Trump, Politics is All About Social Media," San Francisco Chronicle, July 31, 2018.

We answer both questions by analyzing the balance of fact versus opinion statements in the tweets of all major party presidential candidates throughout the course of the 2016 and 2020 elections. We separately analyze President Trump since 2016, because he was, in some sense, campaigning for reelection. We also look at audience engagement with each type of tweet in order to test how supply and demand interact. Our theoretical argument takes as its starting point two empirical findings in the prior literature; one showing how politicians use social media differently from other campaign media, clearly seeking to engage their audience and drawing benefits if they do [@grant_2010; @lee_2012; @graham_2013; @graham_2016; @kreiss_2016; @kreiss_2016a; @lyons_2016], and the other demonstrating that their audiences are far from a random sample of the general public [@conover_2011; @barthel_2015; @pewresearchcenter_2017].

Our central contention is that candidates supply what members of their social media audience implicitly demand when they decide to follow a candidate on Twitter: the politician's subjective take on the world. The very act of choosing to follow a candidate signals that a follower wants to hear from that candidate directly about her values, positions, preferences, and antagonisms, signing up to read her subjective views rather than using the nearly infinite informational resources of the internet to learn objective facts about her. The social media audience, then, should prefer opinions to factual claims. Candidates should shape the supply of their messages to match the demands of their audience, providing more opinion than fact in order to get their online followers to engage with their Twitter feeds. Finally, the immediate feedback mechanisms that Twitter provides -- candidates can see how many "likes" and retweets any fact or opinion receives -- should point them toward sending more of the types of messages that their followers find engaging. We expect to see the rate of opinion versus factual tweets grow over the course of the campaign, especially as candidates who undersupply opinion, relative to other competitors, adjust their strategies.

In order to test the hypotheses suggested by this argument, we construct an original dataset of XXX tweets sent by YY major party candidates during the 2016 and 2020 US presidential campaign. Taking a supervised learning approach to this large-scale text analysis [see @grimmer_2013; @peterson_2018], we present a novel measure categorizing tweets as either factual claims or opinions. Examples of fact-claiming tweets, which need not be in fact true, include John Kasich's "A top ten state for job creation. Wages growing faster than the nation. Record number of businesses" or Carly Fiorina's "In fact, single mothers earn \$19,000 dollars less per year than married mothers. #empoweredwomen #ceidinner." Opinions include Hillary Clinton's "In America, we lift each other up. We build bridges, not walls. We stand together because we're stronger together" and Donald Trump's "Hillary Clinton doesn't have the strength or the stamina to MAKE AMERICA GREAT AGAIN! #AmericaFirst." Working with a team of ten research assistants who coded 8,363 tweets by hand, we demonstrate strong levels of intercoder reliability for this new measure. We then train machine learning algorithms to replicate these human coding decisions, accurately reproducing both our coding of fact versus opinion and measures of the sentiment, ideology, and subject matter of each tweet. After validating this approach, we classify all of these characteristics for the full corpus of tweets. We also gather data on the level of engagement with each tweet, either through audience likes or retweets, along with the size of each candidate's Twitter following at the time the tweet was sent.

This approach allows us to chart the prevalence of fact versus opinion tweets throughout the 2016 and 2020 elections. We find, consistent with our predictions, that it is opinion that a politician's online audience finds most engaging, and that the presidential candidates supplied more of it, especially as the campaign wore on. We calculate the rate of fact versus opinion tweeting for each candidate and chart it over time, showing that Donald Trump spearheaded but was not alone in using social media to set forth his views rather than to inform -- every candidate in the race tweeted their opinions more often than facts. Leveraging a feature of Twitter that allows us to measure the instantaneous reaction of a political audience to elite messages, we examine the impact of fact versus opinion strategies on liking and retweeting rates. Opinions earn likes or go viral at far higher rates than facts for nearly every major candidate. In within-candidate models that control for the ideology, sentiment, and subject matter of a tweet, we show that followers engage with opinions much more often than they do with facts, a trend that held as strongly among Hillary Clinton, Bernie Sanders, and Ted Cruz's followers as it did among Donald Trump's audience. Opinions with a negative sentiment win more engagement than neutral or positive opinions, across all candidates. We also show that the demand for opinion over facts has remained steady over the course of the Trump presidency.

Twitter use by political leaders around the world has become an increasingly promising and vital arena to study [@golbeck_2010; @bruns_2013; @graham_2013; @himelboim_2013; @mejova_2013; @straus_2013; @evans_2014; @kruikemeier_2014; @barbera_2015; @bhattacharya_2015; @evans_2016; @kreiss_2016; @kreiss_2016a; @mcgregor_2016; @mcgregor_2017; @meeks_2016; @metzger_2016; @steinert-threlkeld_2017]. To this burgeoning literature, we contribute a new argument, a novel measure of the content of tweets, and a set of findings showing how supply and demand interact to bring more opinions than facts into America's new digital civic discourse.

## Theory

### Supply: How Do Candidates Use Fact and Opinion?

Presidential candidates began using Twitter as a key part of their communications strategies in 2012 [@conway_2013; @murthy_2015; @kreiss_2016; @kreiss_2016a]. Tt took on heightened prominence in the 2016 campaign, especially through the unique candidacy of Donald Trump, who used his \@realDonaldTrump handle to send 7,792 tweets from July 1, 2015 through Election Day in November 2016, and another 3,079 between the election and March 2, 2018. Yet Trump and his communications team were not alone in making Twitter a central part of their campaign. Hillary Clinton was the most prolific campaign tweeter, sending 8,937 tweets, Bernie Sanders sent 7,794, Ted Cruz sent 8,500, and John Kasich sent 4,384. With Twitter audiences that ranged from 14.4 million followers for Donald Trump to 10.3 million for Hillary Clinton and 3.9 million for Bernie Sanders by Election Day, this media provided an avenue for candidates (and their communications staffs, who often tweet on their behalf) to speak both to media and to a massive number of voters directly. While tweet volume certainly does not predict electoral success [@conway_2013; @mcgregor_2017], Trump's rapid rise suggests that using this medium effectively can be part of a successful campaign strategy. Twitter has taken its place alongside the stump speech and the television commercial as a staple of presidential campaigning. Maintaining a vigorous social media presence is now a central aim of any campaign, with candidates devising intentional online strategies. What, then, do candidates set out to accomplish when they send a tweet?

One way to think about Twitter is that it simply provides an additional medium for candidates to get out their campaign message. If politicians' tweets are nothing more than campaign broadcasts sent through another medium, they should invest in the same sort of rhetorical portfolio that they do on the stump and in their advertisements. Presidential candidates will introduce their biographies, drive home the key message that motivates their candidacy, lay out their stances on major issues, and contrast themselves with their opponents. And, in the words of @mayhew_1974, they will claim credit and take positions. All of these can be accomplished either through factual claims or by statements of opinion.

For instance, Republican candidate Gov. Chris Christie of New Jersey tweeted "I fight for what I believe in and make sure that government works for the people..." in January 2016 in the weeks leading up to the Iowa Caucuses. Contrast that with the more concretely factual credit claim by fellow GOP candidate and former Governor of Arkansas Mike Huckabee who tweeted in November 2015, "As Governor, I passed first broad-based tax cut in the history of the state #GOPDebate #ImWithHuck." Similarly, politicians can also take positions with an opinion, such as Hillary Clinton's statement that "Your loved ones deserve the best care without you having to worry about your paycheck." In an example of factual position taking, Clinton tweeted that she "...has only received one F in her life and she's proud of it: from the NRA. #DemDebate."

Yet a more sophisticated view of how to use Twitter in politics highlights its comparative advantages relative to other media: Twitter allows candidates to send direct and personal messages that invite their audiences to engage, either through "liking" or retweeting a tweet[^2]. Candidates can tweet straight from their phones to the devices of their followers, who then can immediately like or retweet what they read. This prospect for engagement is critical. Studies of politicians' tweets in the UK and in the Netherlands [@graham_2013; @graham_2016] show that many leaders use this new medium to interact with individual Twitter followers or to invite broad engagement. Our argument is that when candidates use Twitter, they should, more than in their other forms of campaign communication, speak in the style most likely to elicit engagement[^3]. The payoffs to engagement are clear: both experimental [@lee_2012; @lyons_2016] and observational [@grant_2010] evidence shows that politicians are evaluated more favorably by voters when they post interactive tweets[^4]. And because candidates get real-time feedback on how engaging their messages are, the savviest candidates will supply the types of messages that their audience demands.

[^2]: There are two ways that people can engage on Twitter. The first is "liking" a tweet, which audience members typically do only if they approve of it and do not mind having others who follow them on Twitter seeing the original tweet. Second, they can retweet it, either because they like it and want their own followers to see it or because they simply want to start a discussion about it. In this paper, we are theorizing about engagement broadly, so all of our hypotheses point toward doing more of both behaviors. The empirical correlation between likes and retweets is 0.91 in our dataset.

[^3]: Of course, it is not a certainty that politicians will supply what their audience demands: Hemsley and Jackson's (2018) correlational analysis of 4,754 tweets sent by candidates in the final three months of 2016 presidential election finds a mismatch between the topics that candidates tweeted about and the topics that their audiences retweeted most often.

[^4]: @lyons_2016 provide experimental evidence showing that respondents evaluate politicians more favorably when politicians post interactive tweets than when they broadcast their positions through one-way communication. @lee_2012 show in an experiment that when politicians have highly interactive Twitter accounts, certain types of respondents have more positive evaluations of the politician and report a stronger intention to vote for him. [@grant_2010, 579] show that Australian politicians "are attempting to use Twitter for political engagement, though some are more successful in this than others" and that "Those who use Twitter to converse appear to gain more political benefit from the platform than others."

Our contention is that politicians will supply the types of messages -- either factual claims or opinions -- that their Twitter followers demand. In the next theoretical section, we make a substantive argument that the social media audience should exhibit a stronger demand for subjective opinions than for opinions than for objective facts. Our data on audience engagement -- measured via likes and retweets -- is strongly consistent with this prediction. Consequently, because we expect strategic politicians to anticipate the demands of their audience, presidential candidates should tweet more opinions than factual claims.

*Hypothesis 1: Presidential candidates, seeking to tweet out what is most engaging to their audience, will supply more opinion than fact on Twitter.*

### Demand: Twitter Followers Seek Opinions

What are people looking for when they choose to follow a politician on Twitter? We contend that this decision is driven by their desire to hear directly from a candidate. When they follow a politician, they may or may not support her, but they are interested in hearing what she has to say, rather than simply reading objective information about her from outside sources. What these followers demand, then, is her personal perspective; the very act of choosing to follow a candidate is akin to signing up to read her subjective views rather than using the nearly infinite informational resources of the internet to learn objective facts about her. Selecting into a candidate's Twitter following signals, we argue, that audience members want to find out whether they are a subjective fit with the candidate. Because of this, they should demand more statements of opinions than factual claims, because it gives them the information that they want and that they can get most directly from the candidate's own feed.

This argument is based on our recognition that a politician's social media audience is by no means a random or representative sample of the general public. While the overall Twitter audience is quite broad, with 22 percent of U.S. adults using Twitter [@pewresearchcenter_2019], it is not perfectly reflective of the electorate, and the audience grows narrower and less representative when one focuses on those who follow politics and then those who choose to follow particular politicians. According to the 2019 Pew Research Center study, Twitter users are younger and more likely to be college graduates than the general population. An earlier study found that there are also racial and ethnic differences, with African-Americans more likely to be on Twitter than white Americans [@pewresearchcenter_2017]. Even among Twitter users, those who follow politicians are a subsample: in @barthel_2015 study, 12 percent of Twitter users followed civic or political leaders (compared to the 35 percent who followed entertainment or sports figures). The subsets of Twitter users who follow candidates with particular ideological stripes are smaller and likely to be even more distinct in their characteristics. While we have not seen direct studies of the ideology of these followers, @barbera_2015 argues convincingly that they share an ideological affinity with the leaders whom they follow, and works by @conover_2011 and @bakshy_2015 show that social media audiences are politically polarized[^5].

[^5]: Specifically, @conover_2011 show that the social networks of Twitter users who tweet about politics are polarized; they retweet the posts of users whose ideologies they agree with, and @bakshy_2015 show that Facebook users choose to consume News Feed articles that fit with their ideological leanings.

There is a strong selection process, then, that constructs a politician's Twitter audience. These are people who have chosen to join Twitter, to use it to learn about politics, and then to follow a specific politician. These decisions, we argue, tells us something about them that is the key to predicting how they will react to different messages. If they wanted to get just the facts about a candidate, they could Google her or use the plethora of internet resources such as votesmart.org to read about her biography, positions, and interest group ratings. For encyclopedic information like this, they can turn to reliable third-party sources, or at least the media that they trust. They can even get factual information on Twitter. @rosenstiel_2015 show that 86% of Twitter users use it to read news, with 73% following individual journalists online. There are many online avenues through which to gather factual information about candidates.

Why might they turn directly to a candidate by following her on Twitter? They would do so, we argue, because they want to get her take on the world, to hear about how she views events in the news or in politics. The act of choosing to follow a politician reveals that they are interested in her subjective view of the world, rather than simply objective facts about her. They want to hear what she stands for, and learn whether her view of the world meshes with their own. Because of what they are searching for, they should be more likely to engage with a politician when she provides them with what they want: her opinions about the world. This logic that the Twitter audience self-selects forms the primary basis for our prediction here. The comparative advantage that a follower gets from subscribing to a candidate's personal Twitter feed is that the candidate can provide her direct, unvarnished, and immediate opinions. Those who decide to follow a candidate, then, should be more receptive to opinions than to facts.

*Hypothesis 2: Twitter users who choose to follow politicians should be more likely to engage with an opinion tweet than a factual tweet. This should be true for both retweets and likes.*

### What types of opinions should lead to the most engagement?

Opinions can be expressed in qualitatively different ways, and it is quite possible that some forms of opinion are more engaging to Twitter followers than others. A rhetorical characteristic that could be especially important in political discourse is the sentiment of an opinion. Using the same strategy of having human judgments on thousands of tweets guide machine learning algorithms to categorize tens of thousands, we identify whether the sentiment of a tweet is positive, negative, or neutral as we judge whether it is fact or opinion. For instance, tweets that express positive opinions include Donald Trump's "Our biggest problems are solved by growth. We need a President who is a job creator. Let's Make America Great Again!" and Hillary Clinton's "Hillary has spent decades fighting for veterans, members of the military, and their families." Negative opinion tweets include Bernie Sanders' "What would a Donald Trump presidency mean for the people of this country? I think it would be an absolute disaster, beyond a disaster," and John Kasich's "How can \@realDonaldTrump's insults & anger make him worthy of the same job as Washington, Lincoln and Reagan?" A tweet with a neutral opinion is Hillary Clinton's "As flotus said the choice in this election is about who will have the power to shape our children for the next four years of their lives."

Which type of opinion should be most engaging? Since our theory of supply and demand provides no clear guidance here, we are agnostic, but lay out three possibilities suggested by different strands of the existing literature. One possibility is that an opinion should be most engaging when there is any clear unneutral sentiment contained in it. Â Neutral opinions do not tell us what a candidate stands for or stands against, and thus should not resonate. @stefan_2012 find that, for politically relevant tweets, a tweet is more likely to be retweeted when it contains more words with an affective dimension, whether that sentiment is positive or negative.

*Hypothesis 3a. An opinion tweet will be more engaging when it expresses any type of sentiment, relative to a neutral sentiment.*

Another possibility is positive opinions may be particularly appealing and engaging because followers could be drawn to politicians more because of what they stand for rather than stand against. One can support Donald Trump because of his views on immigration and taxes, even if one does not hate the New York Times. We lay out another possible expectation based on the logic that what members of a political constituency have in common is what they agree on rather than we they oppose.

*Hypothesis 3b. An opinion tweet will be more engaging when it expresses a positive sentiment, relative to a neutral or negative sentiment.*

A contrasting theory borrows from the logic of work by @abramowitz_2016 on "negative partisanship," which shows that partisans in American politics have increasing negative views of their political opponents. In this line of thinking, people form political views based on what they oppose, and look to find commonality in what politicians or position they detest. This evokes the way politicians use Twitter in other countries: [@vankessel_2016, p. 594] study of tweets by political groups in the Netherlands shows that both right and left-wing populist parties use tweets primarily as an oppositional tool "to give form to their adversarial rhetoric" against mainstream parties.

*Hypothesis 3c. An opinion tweet will be more engaging when it expresses a negative sentiment, relative to a neutral or positive sentiment.*

### Does Demand Reshape Supply?

Finally, we lay out one clear implication of the logic in our argument, that politicians supply more opinion on Twitter because it is the type of rhetoric that their audience demands. If this mechanism is at work, we should see some evidence of a feedback loop through which candidates learn from and respond to their audience's preferences over the course of a campaign. Of course, the savviest politicians might anticipate audience demand, providing more opinion than fact from the start. If they perform well, their competitors may learn from them and emulate their patterns of tweeting (Kreiss 2016a shows that Republicans began to emulate Barack Obama's social media strategies after their demonstrated success in 2012). Or a more direct feedback mechanism can be at work. Candidates who do not give their audience enough of what they want will be able to learn from the likes and retweets of their tweets about what works and what does not work, shifting their balance of tweets away from factual claims and toward more opinion as the campaign progresses. Hypothesis 4: Learning from feedback of Twitter engagement, candidates will supply more of what their audience demands, shifting from factual claims toward opinion tweets over the course of a campaign.

*Hypothesis 4: Learning from feedback of Twitter engagement, candidates will supply more of what their audience demands, shifting from factual claims toward opinion tweets over the course of a campaign.*

## Method: Measuring Fact vs. Opinion in Tweets

To test these hypotheses, we need to categorize each of the tens of thousands of tweets from 2016 presidential candidates as a "factual claim" or an "opinion." We also need to make other judgments about tweets -- measuring the ideological positions that they convey, the sentiments that they connote, the policy areas that they address, and what they ask their audience to do -- to hold constant these characteristics of communication and thus isolate the impact of fact vs. opinion on engagement. Prior works generally take one of two approaches to coding the content of tweets.

Some scholars use their expertise or research assistants to hand code tweets [@golbeck_2010; @graham_2013; @graham_2016; @himelboim_2013; @mejova_2013; @evans_2013; @meeks_2016] while others substitute artificial intelligence for human judgment to categorize the characteristics of massive numbers of tweets [@grant_2010; @bruns_2013; @kruikemeier_2014; @bhattacharya_2015; @murthy_2015; @obschonka_2017] or use the social networks of Twitter followers to infer the ideology of politicians [@barbera_2015; @barbera_2015a; @king_2015].Because we needed to measure the complex concept of fact versus opinion, and to do so for a large volume of tweets, we leveraged the strengths of both approaches through "supervised learning": we worked with a team of research assistants over 15 months to hand-code 8,363 tweets, then used these categorizations to train machine learning algorithms to classify the full set of about XXXX tweets sent before and during the campaign by all candidates, and, after the election, by President Trump. In this section, we detail our approach and present validity checks for our measurement strategy.

The key idea behind a supervised learning approach, which @mcgregor_2016 also apply to political tweets, is to begin with a small sample from the full set of texts being studied, reading and coding them intensively by hand. The final step is training and testing machine learning algorithms on this small dataset before using the algorithms to classify the entire corpus. Starting in October 2015, we used Twitter's public API to download tweets from each major party candidate's account. We also created scripts that automatically downloaded all tweets from these 23 accounts for candidates in the 2016 race from that point onward, creating a complete dataset of tweets running through July 2018. For the 2020 race, we repeated the same process for the 31 candidates who announced a bid for the presidency in either the Republican or Democratic Party primary. We began downloading tweets from the date the candidate announced until January 7th 2021. In addition, we retrieved the candidate's previous 3,200 tweets--the maximum previously available from Twitter's REST API--from the day of their announcement. For all `r 23+31` candidate campaigns across the two races, this gives us uninterrupted coverage between October 2015 and January 2021.

Beginning in June 2016 and continuing through August 2017[^6], we worked with a team of eight graduate and undergraduate research assistants to categorize these tweets. Our coders read only the text of the tweet, receiving no information about who sent it. We created a codebook outlining operational definitions of the concepts that we sought to measure as well as the types of tweets that would fit into each category. We met twice weekly to discuss the concepts and to debate how tweets should be categorized, updating our codebook, contained in Appendix 1, with these difficult cases. This iterative approach follows the best practice recommended by [@grimmer_2013,p. 277].

[^6]: Because we began coding tweets before the campaign ended, after drawing a random sample from our full set of tweets generated by June 2016, we drew additional random samples of tweets from the summer and fall of 2016 and then through June of 2017 for President Trump so that the distribution across time of the tweets in our hand-coded sample reflected the timing of the full corpus of tweets.

Our primary task was to separate fact from opinion. Scholars have long drawn this distinction, with [@shell_1967, p. 5] characterizing it this way: "facts are existing bits of known and verifiable information while opinions, even though based on facts, transcend the absolute certainty of facts and incorporate varying degrees of speculation, confidence, and judgment." Our codebook defines "opinions" as statements of values, speculations about futures events, or judgments, and "factual claims" as assertions that could be verified as true or false. Importantly, they do not need to be true; factual claims are statements that can be fact-checked, not necessarily ones that survive fact-checking. When a tweet contained at least one clear opinion, even if combined with a factual claim, we categorized that tweet as an opinion.

In @tbl-example_tweets, we provide examples tweets that we categorized as fact or opinion, along with examples of two of our other key measures, "ideology" and "sentiment." Another set of that we coded variables put each tweet into one of the ten subject areas, using the topic list created and applied to democracies around the world by the Policy Agendas Project (2017)[^7]. We also gather additional variables to serve as controls in our multivariate models, to isolate the impact of opinions.

[^7]: Because several of the topic areas created by the Policy Agendas Project -- agriculture, labor, energy, transportation, social welfare, housing, domestic commerce, technology, foreign trade, international affairs, public lands, culture -- feature zero or very few tweets, we consolidated this handful of tweets into related topic areas.


|  **Is the Tweet a Factual Claim or an Opinion?**                                                                                                                                                 |                                    |
|:-----------------------------------|:-----------------------------------|
| **Factual Claim**                                                                                                                                                                                                     | **Opinion**                                                                                                          |

| A top ten state for job creation. Wages growing faster than the nation. Record number of businesses.                                                                                                                  | In America, we lift each other up. We build bridges, not walls. We stand together because we're stronger together. |
| In fact, single mothers earn \$19,000 dollars less per year than married mothers. #empoweredwomen #ceidinner                                                                                                          | Hillary Clinton doesn't have the strength or the stamina to MAKE AMERICA GREAT AGAIN! #AmericaFirst                |
| Hundreds turned out this morning in Fairfax to hear Gov. Kasich's message of strength and optimism.In America, we lift each other up. We build bridges, not walls. We stand together because we're stronger together. | I am the only one who stood up to 100K protesters, stood up to the big union bosses. I'll stand up to Washington.     |

: Examples of Variables and Tweets in Each Category {#tbl-example_tweets}

How often did coders agree on their categorization when members of our research team independently coded the same tweets? In Table \@ref(tab:YYYY), we report measures of intercoder reliability for all of our variables, demonstrating generally strong levels of agreement. This reliability analysis is based on a set of 3,634 tweets, which we assigned to overlapping pairs of coders so that each coders and then the Cohen's kappa and Krippendorff's alpha, measures of how much more likely the researchers are to agree than we would expect by pure chance. Our rates of agreement range from 72% on our three-category sentiment measure to near perfect agreement on our policy areas, with the Cohen's kappa measures ranging from "fair" to "almost perfect" agreement levels for all but one of our variables[^8].

[^8]: For our key measure of fact versus opinion, the coders agreed 75% of the time. While imperfect, this is a relatively high score for a new and inherently subjective concept to measure. The kappa of 0.49, indicates a "moderate" level of agreement, according to [@landis_1977, p. 165]. Is this strong enough? According to [@hallgren_2012, p.6], "acceptable IRR estimates will vary depending on the study methods and the research question." Importantly, we use the factual claim versus opinion categorization as an independent variable in our analyses. Any random measurement error in it will lead to attenuation bias in our models [@stefanski_2000], understating the true impact of opinion. The imperfect measurement of this concept, then, should make us more confident in our later findings about its impact on engagement.

Our final step to creating our dataset was to use all of the 8,363 tweets that humans coded by hand to train machine learning algorithms. We began by holding out 725 of these as a final testing set, providing the check on the performance of these algorithms that we report in Table \@ref(tab:ZZZZZZ).

## References
